{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **#YouTubeScrap: A comprehensive tool for scraping YouTube data and transcript**\n",
        "\n",
        "✅ This code, developed by [Ergon Cugler de Moraes Silva](https://github.com/ergoncugler) and [Isabela Rocha](https://www.linkedin.com/in/rocha-isabela/) (Brazil), aims to facilitate the `scraping, analysis, and organization of YouTube video data`. It is designed to extract information from video links, search queries, and metadata directly into Google Sheets. Key features include extracting video metrics (e.g., `title, views, likes, duration, channel details`), processing `transcripts` (when available), and allowing search queries filtered by date ranges. This tool is particularly useful for researchers, analysts, and content creators looking to gather and analyze data from YouTube efficiently and systematically.\n",
        "\n",
        "✅ **The primary functionalities of the code include:** **1. Automated Worksheet Setup:** Automatically creates or loads a Google Sheet with predefined column headers, ensuring all extracted data is structured for analysis / **2. Search YouTube Videos:** Performs video searches based on user-defined queries, filtering results by date ranges and limits, and saves video links directly to the worksheet / **3. Extract Video Metrics:** Processes individual video links to gather detailed metadata, including channel name, number of subscribers, view count, like count, video tags, upload date, and more / **4. Extract Transcripts:** Automatically retrieves video transcripts (in multiple languages) and adds them to the worksheet for content analysis, when available.\n",
        "\n",
        "✅ All extracted data is stored directly in a Google Sheet, making it accessible, shareable, and ready for further processing. The integration of tools like yt_dlp and YouTubeTranscriptAPI ensures accuracy, flexibility, and compliance with modern data formats. Researchers and analysts can benefit from its scalability and efficient handling of large datasets, avoiding the limitations of manual extraction.\n",
        "\n",
        "✅ The code is open-source and freely available for use at [GitHub repository link (to be added)]. While the code can be freely modified and adapted, users are responsible for ensuring their usage complies with YouTube's terms of service and relevant data privacy regulations.\n",
        "\n",
        "## **#Applications and Use Cases**\n",
        "✅ The YouTubeScrap has a wide range of applications for research and analysis: **1. Academic Research:** Useful for exploring public discourse, analyzing political content, studying misinformation, or tracking media trends on YouTube / **2. Media Analysis:** Enables the extraction of video trends, engagement metrics, and channel growth for journalists and media professionals / **3. Social Science and Communication Studies:** Facilitates the study of digital communities, propaganda, and content production on YouTube platforms / **4. Content Creators and Marketers:** Provides insights into competitor performance, video optimization, and viewer engagement for strategic planning.\n",
        "\n",
        "✅ To credit this academic work and the scraping code, it is recommended to cite: **SILVA, Ergon Cugler de Moraes; ROCHA, Isabela. *TelegramScrap: A comprehensive tool for scraping Telegram data*. (Dez) 2024. Available at: [https://github.com/ergoncugler/web-scraping-youtube/](https://github.com/ergoncugler/web-scraping-youtube/).**"
      ],
      "metadata": {
        "id": "TgkUGGAKrPST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **1. [ Required ] Create or load a Google Sheet** { display-mode: \"form\" }\n",
        "\n",
        "# @markdown **This cell is required and must be run first** to initialize the Google Sheet used for storing data. It will either:\n",
        "# @markdown 1. Create a new Google Sheet `sheet_name` with predefined columns, or\n",
        "# @markdown 2. Load an existing sheet named in the `sheet_name` variable.\n",
        "\n",
        "# @markdown You will need to **authorize access to Google Drive** for this to work. This is secure, and no data will be exposed or shared beyond this script. Once run, a link to your worksheet will be provided. Without this step, subsequent cells will not work correctly, as you will not have the spreadsheet available to allocate metrics and transcripts.\n",
        "\n",
        "\n",
        "!pip install -q yt-dlp youtube-transcript-api gspread tqdm scrape-youtube\n",
        "\n",
        "sheet_name = \"brazilian_example\" # @param {type:\"string\"}\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "from google.auth import default\n",
        "\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "COLUMN_HEADERS = [\n",
        "    \"Link\", \"ID\", \"Title\", \"Full Title\", \"Description\", \"Channel Name\", \"Channel ID\",\n",
        "    \"Channel URL\", \"Timestamp\", \"Publish Date\", \"Channel Name (Alt)\", \"Channel ID (Alt)\",\n",
        "    \"Channel URL (Alt)\", \"Subscribers\", \"Is Verified\", \"Location\", \"Length (s)\",\n",
        "    \"Views\", \"Likes\", \"Dislikes\", \"Reposts\", \"Comments\", \"Tags\", \"Thumbnail\", \"Transcript\"\n",
        "]\n",
        "\n",
        "def load_or_create_sheet(sheet_name, headers):\n",
        "    try:\n",
        "        sheet = gc.open(sheet_name)\n",
        "    except gspread.SpreadsheetNotFound:\n",
        "        sheet = gc.create(sheet_name)\n",
        "        print(f\"Worksheet '{sheet_name}' created successfully.\")\n",
        "    worksheet = sheet.get_worksheet(0) or sheet.add_worksheet(title=\"Sheet1\", rows=\"100\", cols=\"26\")\n",
        "    current_headers = worksheet.row_values(1)\n",
        "    if current_headers != headers:\n",
        "        worksheet.insert_row(headers, 1)\n",
        "    print(f\"Access your worksheet here: https://docs.google.com/spreadsheets/d/{sheet.id}/edit\")\n",
        "    return worksheet\n",
        "\n",
        "worksheet = load_or_create_sheet(sheet_name, COLUMN_HEADERS)\n"
      ],
      "metadata": {
        "id": "3uzCqIiClvHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **2. [ Optional ] Search YouTube videos and save links**  *(with the worksheet already created in step 1)*{ display-mode: \"form\" }\n",
        "\n",
        "# @markdown **This step is optional if you prefer to add links manually in the sheet.** Use this cell to **search YouTube videos** based on up to 5 queries and save their links in the Google Sheet.\n",
        "# @markdown - Define search terms in `query_1`, `query_2`, etc. Use operators like `-` to exclude terms (e.g., `'election -trump'`).\n",
        "# @markdown - Set `max_results` to limit the number of videos fetched per query (default: 10).\n",
        "# @markdown - Use `start_date` and `end_date` to restrict the search to a specific date range. Leave these blank to fetch all available videos.\n",
        "\n",
        "query_1 = \"'presidente lula' -bolsonaro -eleição\" # @param {type:\"string\"}\n",
        "query_2 = \"election usa -biden -kamala -trump\" # @param {type:\"string\"}\n",
        "query_3 = \"\" # @param {type:\"string\"}\n",
        "query_4 = \"\" # @param {type:\"string\"}\n",
        "query_5 = \"\" # @param {type:\"string\"}\n",
        "max_results = 10 # @param {type:\"slider\", min:1, max:1000, step:1}\n",
        "start_date = '2020-01-01' # @param {type:\"date\"}\n",
        "end_date = '2024-12-31' # @param {type:\"date\"}\n",
        "\n",
        "import scrapetube\n",
        "from yt_dlp import YoutubeDL\n",
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "queries = [query_1, query_2, query_3, query_4, query_5]\n",
        "queries = [q for q in queries if q.strip()]  # Remove empty queries\n",
        "\n",
        "date_start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
        "date_end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
        "\n",
        "def get_video_details(video_id):\n",
        "    url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
        "    ydl_opts = {\"quiet\": True}\n",
        "    with YoutubeDL(ydl_opts) as ydl:\n",
        "        info = ydl.extract_info(url, download=False)\n",
        "        return {\n",
        "            \"title\": info.get(\"title\", \"Unknown\"),\n",
        "            \"upload_date\": info.get(\"upload_date\", \"00000000\"),\n",
        "            \"link\": url,\n",
        "        }\n",
        "\n",
        "def video_matches_date(video, date_start, date_end):\n",
        "    try:\n",
        "        upload_date = datetime.strptime(video[\"upload_date\"], \"%Y%m%d\")\n",
        "        return date_start <= upload_date <= date_end\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def search_youtube_to_sheet(worksheet, queries, max_results, date_start, date_end):\n",
        "    next_row = len(worksheet.col_values(1)) + 1\n",
        "    for query in tqdm(queries, desc=\"Searching YouTube\"):\n",
        "        videos = scrapetube.get_search(query)\n",
        "        results = []\n",
        "        for video in tqdm(videos, desc=f\"Processing '{query}'\", leave=False):\n",
        "            if len(results) >= max_results:\n",
        "                break\n",
        "            video_id = video[\"videoId\"]\n",
        "            video_details = get_video_details(video_id)\n",
        "            if video_matches_date(video_details, date_start, date_end):\n",
        "                results.append(video_details)\n",
        "                worksheet.update_cell(next_row, 1, video_details[\"link\"])\n",
        "                next_row += 1\n",
        "\n",
        "search_youtube_to_sheet(worksheet, queries, max_results, date_start, date_end)\n"
      ],
      "metadata": {
        "id": "KP-Pj2z1Z7uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **3. [ Optional ] Extract metrics from videos** *(with the worksheet already created in Step 1 and links either inserted manually or added in Step 2)* { display-mode: \"form\" }\n",
        "\n",
        "# @markdown Run this cell to **extract metadata** for video links stored in the Google Sheet. Data such as `title`, `views`, `likes`, `channel name`, and `upload date` will be automatically fetched and added to their respective columns. The script processes only links without existing metadata, ensuring efficient updates. Make sure the Google Sheet contains valid YouTube links in the `Link` column before running this cell.\n",
        "\n",
        "\n",
        "from yt_dlp import YoutubeDL\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def extract_video_id(link):\n",
        "    \"\"\"\n",
        "    Extracts the video ID from a YouTube link.\n",
        "\n",
        "    Args:\n",
        "        link (str): Full video URL.\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted video ID.\n",
        "    \"\"\"\n",
        "    match = re.search(r\"v=([^&]+)\", link)\n",
        "    return match.group(1) if match else \"Unknown\"\n",
        "\n",
        "def extract_video_metrics(worksheet):\n",
        "    \"\"\"\n",
        "    Extracts video metrics and updates the worksheet.\n",
        "\n",
        "    Args:\n",
        "        worksheet: Google Worksheet object.\n",
        "    \"\"\"\n",
        "    ydl_opts = {\n",
        "        'quiet': True,\n",
        "        'no_warnings': True,\n",
        "        'skip_download': True,\n",
        "    }\n",
        "\n",
        "    # Collect all links with missing metadata\n",
        "    links = []\n",
        "    i = 2\n",
        "    while worksheet.cell(i, 1).value:\n",
        "        if not worksheet.cell(i, 2).value:\n",
        "            links.append((i, worksheet.cell(i, 1).value))\n",
        "        i += 1\n",
        "\n",
        "    # Iterate over links and process each video\n",
        "    for index, link in tqdm(links, desc=\"Extracting metrics\"):\n",
        "        try:\n",
        "            video_id = extract_video_id(link)  # Extract video ID from the link\n",
        "\n",
        "            with YoutubeDL(ydl_opts) as ydl:\n",
        "                info = ydl.extract_info(link, download=False)\n",
        "\n",
        "            tags = info.get('tags', [])\n",
        "            tags_str = ', '.join(tags) if isinstance(tags, list) else 'Unknown'\n",
        "\n",
        "            # Update worksheet with extracted data\n",
        "            worksheet.update_cell(index, 2, video_id)  # Video ID\n",
        "            worksheet.update_cell(index, 3, info.get('title', 'Unknown'))\n",
        "            worksheet.update_cell(index, 4, info.get('fulltitle', 'Unknown'))\n",
        "            worksheet.update_cell(index, 5, info.get('description', '').replace('\\n', ' '))\n",
        "            worksheet.update_cell(index, 6, info.get('uploader', 'Unknown'))\n",
        "            worksheet.update_cell(index, 7, info.get('uploader_id', 'Unknown'))\n",
        "            worksheet.update_cell(index, 8, info.get('uploader_url', 'Unknown'))\n",
        "            worksheet.update_cell(index, 9, info.get('timestamp', 'Unknown'))\n",
        "            worksheet.update_cell(index, 10, info.get('upload_date', 'Unknown'))\n",
        "            worksheet.update_cell(index, 11, info.get('channel', 'Unknown'))\n",
        "            worksheet.update_cell(index, 12, info.get('channel_id', 'Unknown'))\n",
        "            worksheet.update_cell(index, 13, info.get('channel_url', 'Unknown'))\n",
        "            worksheet.update_cell(index, 14, info.get('channel_follower_count', 'Unknown'))\n",
        "            worksheet.update_cell(index, 15, info.get('channel_is_verified', 'FALSE'))\n",
        "            worksheet.update_cell(index, 16, info.get('location', 'Unknown'))\n",
        "            worksheet.update_cell(index, 17, info.get('duration', 'Unknown'))\n",
        "            worksheet.update_cell(index, 18, info.get('view_count', 'Unknown'))\n",
        "            worksheet.update_cell(index, 19, info.get('like_count', 'Unknown'))\n",
        "            worksheet.update_cell(index, 20, info.get('dislike_count', 'Unknown'))\n",
        "            worksheet.update_cell(index, 21, info.get('repost_count', 'Unknown'))\n",
        "            worksheet.update_cell(index, 22, info.get('comment_count', 'Unknown'))\n",
        "            worksheet.update_cell(index, 23, tags_str)\n",
        "            worksheet.update_cell(index, 24, info.get('thumbnail', 'Unknown'))\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing link {link}: {e}\")\n",
        "\n",
        "print(\"Processing metrics...\")\n",
        "extract_video_metrics(worksheet)\n",
        "print(\"Metrics extracted successfully!\")\n"
      ],
      "metadata": {
        "id": "NBsRinjFZ73x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **4. [ Optional ] Extract video transcripts** *(with the worksheet already created in Step 1 and links either inserted manually or added in Step 2)* { display-mode: \"form\" }\n",
        "\n",
        "# @markdown Use this cell to retrieve **video transcripts** (if available) in supported languages like `English`, `Spanish`, and `Portuguese`. Transcripts will be added to the `Transcript` column in the Google Sheet. If no transcript is found, the entry will be marked as `[no subtitles]`. The script processes only rows without existing transcripts, skipping completed entries.\n",
        "\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def extract_video_transcripts(worksheet):\n",
        "    col_url = worksheet.col_values(1)\n",
        "    urls_to_process = [(idx, url) for idx, url in enumerate(col_url[1:], start=2) if not worksheet.cell(idx, 25).value]\n",
        "\n",
        "    for idx, url in tqdm(urls_to_process, desc=\"Extracting transcripts\"):\n",
        "        video_id = re.search(r\"v=([^&]+)\", url)\n",
        "        video_id = video_id.group(1) if video_id else None\n",
        "        if video_id:\n",
        "            try:\n",
        "                transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en', 'es', 'pt'])\n",
        "                transcript_text = ' '.join([item['text'].replace('\\n', ' ') for item in transcript])\n",
        "                worksheet.update_cell(idx, 25, transcript_text)\n",
        "            except Exception:\n",
        "                worksheet.update_cell(idx, 25, \"[no subtitles]\")\n",
        "\n",
        "extract_video_transcripts(worksheet)\n"
      ],
      "metadata": {
        "id": "DSeTuOT9Z76b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
